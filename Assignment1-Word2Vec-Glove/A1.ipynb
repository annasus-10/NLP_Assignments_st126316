{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4b847c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.12 (main, Oct 28 2025, 11:52:25) [Clang 20.1.4 ]\n",
      "Platform: macOS-26.2-arm64-arm-64bit\n",
      "Torch: 2.9.1\n",
      "MPS available: True\n",
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "import sys, platform, time\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Platform:\", platform.platform())\n",
    "\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"MPS available:\", torch.backends.mps.is_available())\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e914d9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK data paths:\n",
      " - /Users/thetsusann/nltk_data\n",
      " - /Users/thetsusann/Desktop/NLP/Assignment1/.venv/nltk_data\n",
      " - /Users/thetsusann/Desktop/NLP/Assignment1/.venv/share/nltk_data\n",
      " - /Users/thetsusann/Desktop/NLP/Assignment1/.venv/lib/nltk_data\n",
      " - /usr/share/nltk_data\n",
      " - /usr/local/share/nltk_data\n",
      " - /usr/lib/nltk_data\n",
      " - /usr/local/lib/nltk_data\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(\"NLTK data paths:\")\n",
    "for p in nltk.data.path:\n",
    "    print(\" -\", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "556ae5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading into: /Users/thetsusann/nltk_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/thetsusann/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/thetsusann/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     /Users/thetsusann/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, nltk\n",
    "\n",
    "target_dir = nltk.data.path[0]\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "print(\"Downloading into:\", target_dir)\n",
    "\n",
    "nltk.download(\"punkt\", download_dir=target_dir)\n",
    "nltk.download(\"punkt_tab\", download_dir=target_dir)\n",
    "nltk.download(\"reuters\", download_dir=target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fafd779a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "punkt: /Users/thetsusann/nltk_data/tokenizers/punkt\n",
      "punkt_tab: /Users/thetsusann/nltk_data/tokenizers/punkt_tab/english\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.data import find\n",
    "\n",
    "print(\"punkt:\", find(\"tokenizers/punkt\"))\n",
    "print(\"punkt_tab:\", find(\"tokenizers/punkt_tab/english/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82786e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reuters docs: 10788\n",
      "Sanity sentences: 10114\n",
      "Example sentence: ['asian', 'exporters', 'fear', 'damage', 'from', 'rift', 'mounting', 'trade', 'friction', 'between', 'the', 'and', 'japan', 'has', 'raised', 'fears', 'among', 'many', 'of', 'asia']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import reuters\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "fileids = reuters.fileids()\n",
    "print(\"Total Reuters docs:\", len(fileids))\n",
    "\n",
    "SANITY_DOCS = 2000\n",
    "subset_ids = fileids[:SANITY_DOCS]\n",
    "\n",
    "sentences = []\n",
    "for fid in subset_ids:\n",
    "    raw = reuters.raw(fid)\n",
    "    for sent in sent_tokenize(raw):\n",
    "        tokens = [w.lower() for w in word_tokenize(sent)]\n",
    "        tokens = [w for w in tokens if w.isalpha()]\n",
    "        if len(tokens) >= 3:\n",
    "            sentences.append(tokens)\n",
    "\n",
    "print(\"Sanity sentences:\", len(sentences))\n",
    "print(\"Example sentence:\", sentences[0][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b06ea1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw unique words: 12324\n",
      "Vocab size (with <UNK>): 4075\n",
      "Total tokens: 238574\n",
      "UNK tokens: 14365 (6.02%)\n",
      "Example encoded sentence: [1892, 604, 2662, 939, 20, 0, 3190, 49, 2663, 142, 1, 6, 65, 29, 455, 1730, 686, 561, 2, 1731]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Hyperparams for the sanity run (fast + stable)\n",
    "MIN_COUNT = 5\n",
    "MAX_VOCAB = 20000   # cap vocab to keep training fast\n",
    "\n",
    "# Count words\n",
    "word_counts = Counter(w for sent in sentences for w in sent)\n",
    "\n",
    "# Keep words above MIN_COUNT\n",
    "filtered = [(w, c) for w, c in word_counts.items() if c >= MIN_COUNT]\n",
    "filtered.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Cap vocab\n",
    "filtered = filtered[:MAX_VOCAB]\n",
    "\n",
    "# Build mappings (reserve 0 for UNK)\n",
    "word2id = {\"<UNK>\": 0}\n",
    "id2word = {0: \"<UNK>\"}\n",
    "\n",
    "for i, (w, c) in enumerate(filtered, start=1):\n",
    "    word2id[w] = i\n",
    "    id2word[i] = w\n",
    "\n",
    "vocab_size = len(word2id)\n",
    "print(\"Raw unique words:\", len(word_counts))\n",
    "print(\"Vocab size (with <UNK>):\", vocab_size)\n",
    "\n",
    "# Encode sentences as IDs\n",
    "corpus_ids = []\n",
    "unk_count = 0\n",
    "total_tokens = 0\n",
    "\n",
    "for sent in sentences:\n",
    "    ids = []\n",
    "    for w in sent:\n",
    "        total_tokens += 1\n",
    "        wid = word2id.get(w, 0)\n",
    "        if wid == 0:\n",
    "            unk_count += 1\n",
    "        ids.append(wid)\n",
    "    corpus_ids.append(ids)\n",
    "\n",
    "print(\"Total tokens:\", total_tokens)\n",
    "print(\"UNK tokens:\", unk_count, f\"({unk_count/total_tokens:.2%})\")\n",
    "print(\"Example encoded sentence:\", corpus_ids[0][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a91ccf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip_grams shape: (792604, 2)\n",
      "Example pairs (center, context): [[2662 1892]\n",
      " [2662  604]\n",
      " [2662  939]\n",
      " [2662   20]\n",
      " [ 939  604]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "WINDOW_SIZE = 2  # default required by assignment\n",
    "\n",
    "skip_grams = []\n",
    "for sent in corpus_ids:\n",
    "    # skip sentences that are too short\n",
    "    if len(sent) < 2 * WINDOW_SIZE + 1:\n",
    "        continue\n",
    "    for center_i in range(WINDOW_SIZE, len(sent) - WINDOW_SIZE):\n",
    "        center = sent[center_i]\n",
    "        # context within window (exclude center itself)\n",
    "        for j in range(center_i - WINDOW_SIZE, center_i + WINDOW_SIZE + 1):\n",
    "            if j == center_i:\n",
    "                continue\n",
    "            context = sent[j]\n",
    "            skip_grams.append((center, context))\n",
    "\n",
    "skip_grams = np.array(skip_grams, dtype=np.int64)\n",
    "print(\"skip_grams shape:\", skip_grams.shape)  # (num_pairs, 2)\n",
    "print(\"Example pairs (center, context):\", skip_grams[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a678192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centers: torch.Size([256]) torch.int64 mps:0\n",
      "contexts: torch.Size([256]) torch.int64 mps:0\n",
      "sample: [1294, 0, 568, 3369, 40] [0, 31, 120, 0, 26]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "def get_batch(batch_size=BATCH_SIZE, device=device):\n",
    "    idx = np.random.randint(0, len(skip_grams), size=batch_size)\n",
    "    batch = skip_grams[idx]\n",
    "    centers = torch.tensor(batch[:, 0], dtype=torch.long, device=device)\n",
    "    contexts = torch.tensor(batch[:, 1], dtype=torch.long, device=device)\n",
    "    return centers, contexts\n",
    "\n",
    "# quick sanity check\n",
    "c, x = get_batch()\n",
    "print(\"centers:\", c.shape, c.dtype, c.device)\n",
    "print(\"contexts:\", x.shape, x.dtype, x.device)\n",
    "print(\"sample:\", c[:5].tolist(), x[:5].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4144bb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 50/200 loss=7.2239\n",
      "step 100/200 loss=6.6175\n",
      "step 150/200 loss=6.7080\n",
      "step 200/200 loss=6.4783\n",
      "loss first/last: 8.455802917480469 6.478254318237305\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "class SkipGramSoftmax(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        super().__init__()\n",
    "        self.in_embed = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.out_linear = nn.Linear(emb_dim, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, center_ids):\n",
    "        v = self.in_embed(center_ids)         # [B, D]\n",
    "        logits = self.out_linear(v)           # [B, V]\n",
    "        return logits\n",
    "\n",
    "EMB_DIM = 50\n",
    "model_sg = SkipGramSoftmax(vocab_size=vocab_size, emb_dim=EMB_DIM).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_sg.parameters(), lr=0.01)\n",
    "\n",
    "# Tiny sanity training: 200 steps\n",
    "model_sg.train()\n",
    "losses = []\n",
    "for step in range(200):\n",
    "    centers, contexts = get_batch()\n",
    "    logits = model_sg(centers)               # [B, V]\n",
    "    loss = criterion(logits, contexts)       # contexts: [B]\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "    if (step + 1) % 50 == 0:\n",
    "        print(f\"step {step+1}/200 loss={loss.item():.4f}\")\n",
    "\n",
    "print(\"loss first/last:\", losses[0], losses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a95b26ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 500/3000 avg_loss(last500)=6.1983\n",
      "step 1000/3000 avg_loss(last500)=6.0316\n",
      "step 1500/3000 avg_loss(last500)=5.9553\n",
      "step 2000/3000 avg_loss(last500)=5.9014\n",
      "step 2500/3000 avg_loss(last500)=5.8498\n",
      "step 3000/3000 avg_loss(last500)=5.7925\n",
      "\n",
      "Skip-gram Softmax DONE\n",
      "time_sec: 4.2142112255096436\n",
      "final_avg_loss: 5.79247337436676\n",
      "saved: sg_softmax_embeddings.npy (4075, 50)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "FINAL_STEPS = 3000   \n",
    "\n",
    "model_sg.train()\n",
    "t0 = time.time()\n",
    "\n",
    "loss_log = []\n",
    "for step in range(FINAL_STEPS):\n",
    "    centers, contexts = get_batch()\n",
    "    logits = model_sg(centers)\n",
    "    loss = criterion(logits, contexts)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_log.append(loss.item())\n",
    "    if (step + 1) % 500 == 0:\n",
    "        avg = float(np.mean(loss_log[-500:]))\n",
    "        print(f\"step {step+1}/{FINAL_STEPS} avg_loss(last500)={avg:.4f}\")\n",
    "\n",
    "t1 = time.time()\n",
    "train_time = t1 - t0\n",
    "final_avg_loss = float(np.mean(loss_log[-500:])) if len(loss_log) >= 500 else float(np.mean(loss_log))\n",
    "\n",
    "print(\"\\nSkip-gram Softmax DONE\")\n",
    "print(\"time_sec:\", train_time)\n",
    "print(\"final_avg_loss:\", final_avg_loss)\n",
    "\n",
    "# Save embeddings\n",
    "sg_embeddings = model_sg.in_embed.weight.detach().to(\"cpu\").numpy()\n",
    "np.save(\"sg_softmax_embeddings.npy\", sg_embeddings)\n",
    "print(\"saved: sg_softmax_embeddings.npy\", sg_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1758ad30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigram_table size: (200000,)\n",
      "table id min/max: 1 4074\n",
      "contains UNK (0)? False\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Build unigram distribution with 0.75 power (standard for word2vec NEG)\n",
    "power = 0.75\n",
    "\n",
    "# word_counts includes counts for ALL raw words, but NEG should use your final vocab\n",
    "# Let's reconstruct counts aligned to vocab IDs\n",
    "id_counts = np.zeros(vocab_size, dtype=np.int64)\n",
    "\n",
    "# <UNK> count = total unk occurrences\n",
    "id_counts[0] = 0  # keep <UNK> out of negative sampling (common choice)\n",
    "\n",
    "for w, wid in word2id.items():\n",
    "    if wid == 0:\n",
    "        continue\n",
    "    id_counts[wid] = word_counts[w]\n",
    "\n",
    "# Probabilities\n",
    "p = id_counts.astype(np.float64) ** power\n",
    "p_sum = p.sum()\n",
    "p = p / p_sum\n",
    "\n",
    "# Unigram table (controls speed/quality tradeoff)\n",
    "TABLE_SIZE = 200000  # good for our small vocab; bigger = smoother but more memory\n",
    "unigram_table = np.random.choice(np.arange(vocab_size), size=TABLE_SIZE, p=p)\n",
    "\n",
    "print(\"unigram_table size:\", unigram_table.shape)\n",
    "print(\"table id min/max:\", unigram_table.min(), unigram_table.max())\n",
    "print(\"contains UNK (0)?\", (unigram_table == 0).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2851eca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 50/200 loss=14.3470\n",
      "step 100/200 loss=11.6801\n",
      "step 150/200 loss=10.4369\n",
      "step 200/200 loss=9.7720\n",
      "loss first/last: 18.145038604736328 9.77197265625\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "NEG_K = 5\n",
    "EMB_DIM = 50\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "class SkipGramNEG(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        super().__init__()\n",
    "        self.in_embed = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.out_embed = nn.Embedding(vocab_size, emb_dim)\n",
    "\n",
    "    def forward(self, center_ids, pos_context_ids, neg_context_ids):\n",
    "        # center_ids: [B]\n",
    "        # pos_context_ids: [B]\n",
    "        # neg_context_ids: [B, K]\n",
    "\n",
    "        v = self.in_embed(center_ids)                 # [B, D]\n",
    "        u_pos = self.out_embed(pos_context_ids)       # [B, D]\n",
    "        u_neg = self.out_embed(neg_context_ids)       # [B, K, D]\n",
    "\n",
    "        # positive loss: -log sigma(u_pos · v)\n",
    "        pos_score = torch.sum(u_pos * v, dim=1)       # [B]\n",
    "        pos_loss = -torch.log(torch.sigmoid(pos_score) + 1e-9)\n",
    "\n",
    "        # negative loss: -sum log sigma(-u_neg · v)\n",
    "        neg_score = torch.bmm(u_neg, v.unsqueeze(2)).squeeze(2)  # [B, K]\n",
    "        neg_loss = -torch.sum(torch.log(torch.sigmoid(-neg_score) + 1e-9), dim=1)\n",
    "\n",
    "        return (pos_loss + neg_loss).mean()\n",
    "\n",
    "model_neg = SkipGramNEG(vocab_size=vocab_size, emb_dim=EMB_DIM).to(device)\n",
    "optimizer_neg = optim.Adam(model_neg.parameters(), lr=0.01)\n",
    "\n",
    "def get_batch_neg(batch_size=BATCH_SIZE, k=NEG_K, device=device):\n",
    "    idx = np.random.randint(0, len(skip_grams), size=batch_size)\n",
    "    batch = skip_grams[idx]\n",
    "    centers = torch.tensor(batch[:, 0], dtype=torch.long, device=device)\n",
    "    pos = torch.tensor(batch[:, 1], dtype=torch.long, device=device)\n",
    "\n",
    "    # sample negatives from unigram table (fast)\n",
    "    neg_np = unigram_table[np.random.randint(0, len(unigram_table), size=(batch_size, k))]\n",
    "    # ensure neg != pos (rare, but fix)\n",
    "    pos_np = batch[:, 1]\n",
    "    mask = (neg_np == pos_np[:, None])\n",
    "    while mask.any():\n",
    "        neg_np[mask] = unigram_table[np.random.randint(0, len(unigram_table), size=mask.sum())]\n",
    "        mask = (neg_np == pos_np[:, None])\n",
    "\n",
    "    neg = torch.tensor(neg_np, dtype=torch.long, device=device)\n",
    "    return centers, pos, neg\n",
    "\n",
    "# Tiny sanity training: 200 steps\n",
    "model_neg.train()\n",
    "losses = []\n",
    "for step in range(200):\n",
    "    centers, pos, neg = get_batch_neg()\n",
    "    loss = model_neg(centers, pos, neg)\n",
    "\n",
    "    optimizer_neg.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_neg.step()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "    if (step + 1) % 50 == 0:\n",
    "        print(f\"step {step+1}/200 loss={loss.item():.4f}\")\n",
    "\n",
    "print(\"loss first/last:\", losses[0], losses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46af33be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1000/6000 avg_loss(last1000)=4.8793\n",
      "step 2000/6000 avg_loss(last1000)=2.7656\n",
      "step 3000/6000 avg_loss(last1000)=2.4342\n",
      "step 4000/6000 avg_loss(last1000)=2.2937\n",
      "step 5000/6000 avg_loss(last1000)=2.2100\n",
      "step 6000/6000 avg_loss(last1000)=2.1561\n",
      "\n",
      "Skip-gram NEG DONE\n",
      "time_sec: 8.173359870910645\n",
      "final_avg_loss: 2.156061161994934\n",
      "saved: sg_neg_embeddings.npy (4075, 50)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "FINAL_STEPS_NEG = 6000   # NEG is cheaper; this is still quick. If needed, drop to 3000.\n",
    "\n",
    "model_neg.train()\n",
    "t0 = time.time()\n",
    "\n",
    "loss_log = []\n",
    "for step in range(FINAL_STEPS_NEG):\n",
    "    centers, pos, neg = get_batch_neg()\n",
    "    loss = model_neg(centers, pos, neg)\n",
    "\n",
    "    optimizer_neg.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_neg.step()\n",
    "\n",
    "    loss_log.append(loss.item())\n",
    "    if (step + 1) % 1000 == 0:\n",
    "        avg = float(np.mean(loss_log[-1000:]))\n",
    "        print(f\"step {step+1}/{FINAL_STEPS_NEG} avg_loss(last1000)={avg:.4f}\")\n",
    "\n",
    "t1 = time.time()\n",
    "train_time_neg = t1 - t0\n",
    "final_avg_loss_neg = float(np.mean(loss_log[-1000:])) if len(loss_log) >= 1000 else float(np.mean(loss_log))\n",
    "\n",
    "print(\"\\nSkip-gram NEG DONE\")\n",
    "print(\"time_sec:\", train_time_neg)\n",
    "print(\"final_avg_loss:\", final_avg_loss_neg)\n",
    "\n",
    "neg_embeddings = model_neg.in_embed.weight.detach().to(\"cpu\").numpy()\n",
    "np.save(\"sg_neg_embeddings.npy\", neg_embeddings)\n",
    "print(\"saved: sg_neg_embeddings.npy\", neg_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8c5cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed sentences: 1000/10114; cooc_pairs=45976\n",
      "processed sentences: 2000/10114; cooc_pairs=77913\n",
      "processed sentences: 3000/10114; cooc_pairs=102998\n",
      "processed sentences: 4000/10114; cooc_pairs=129811\n",
      "processed sentences: 5000/10114; cooc_pairs=150534\n",
      "processed sentences: 6000/10114; cooc_pairs=171992\n",
      "processed sentences: 7000/10114; cooc_pairs=195341\n",
      "processed sentences: 8000/10114; cooc_pairs=217723\n",
      "processed sentences: 9000/10114; cooc_pairs=237351\n",
      "processed sentences: 10000/10114; cooc_pairs=257348\n",
      "\n",
      "Co-occurrence build DONE\n",
      "num_pairs: 259866\n",
      "time_sec: 0.22613811492919922\n",
      "sample: [((1892, 604), 1.0), ((1892, 2662), 0.5), ((604, 1892), 1.0), ((604, 2662), 1.0), ((604, 939), 0.5)]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "GLOVE_WINDOW = 2  # keep consistent with earlier window unless your prof specifies otherwise\n",
    "\n",
    "t0 = time.time()\n",
    "cooc = defaultdict(float)\n",
    "\n",
    "for si, sent in enumerate(corpus_ids):\n",
    "    n = len(sent)\n",
    "    for i, wi in enumerate(sent):\n",
    "        # context window\n",
    "        start = max(0, i - GLOVE_WINDOW)\n",
    "        end = min(n, i + GLOVE_WINDOW + 1)\n",
    "        for j in range(start, end):\n",
    "            if j == i:\n",
    "                continue\n",
    "            wj = sent[j]\n",
    "            dist = abs(j - i)\n",
    "            # standard GloVe weighting: 1/dist\n",
    "            cooc[(wi, wj)] += 1.0 / dist\n",
    "\n",
    "    if (si + 1) % 1000 == 0:\n",
    "        print(f\"processed sentences: {si+1}/{len(corpus_ids)}; cooc_pairs={len(cooc)}\")\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"\\nCo-occurrence build DONE\")\n",
    "print(\"num_pairs:\", len(cooc))\n",
    "print(\"time_sec:\", t1 - t0)\n",
    "\n",
    "# peek a few entries\n",
    "sample_items = list(cooc.items())[:5]\n",
    "print(\"sample:\", sample_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c80fabb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training pairs N: 259866\n",
      "epoch 1/5 avg_loss=1.9079\n",
      "epoch 2/5 avg_loss=1.1289\n",
      "epoch 3/5 avg_loss=0.8688\n",
      "epoch 4/5 avg_loss=0.7129\n",
      "epoch 5/5 avg_loss=0.6059\n",
      "\n",
      "GloVe DONE\n",
      "time_sec: 1.7182388305664062\n",
      "saved: glove_embeddings.npy (4075, 50)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "\n",
    "# Convert cooc dict to arrays (so training loop is fast)\n",
    "pairs = np.array(list(cooc.keys()), dtype=np.int64)        # [N, 2]\n",
    "counts = np.array(list(cooc.values()), dtype=np.float32)   # [N]\n",
    "N = len(counts)\n",
    "print(\"Training pairs N:\", N)\n",
    "\n",
    "# GloVe hyperparams (safe defaults)\n",
    "EMB_DIM = 50\n",
    "X_MAX = 100.0\n",
    "ALPHA = 0.75\n",
    "BATCH_SIZE_G = 2048\n",
    "EPOCHS_G = 5\n",
    "LR_G = 0.05\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "class GloVe(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        super().__init__()\n",
    "        self.w = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.w_tilde = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.b = nn.Embedding(vocab_size, 1)\n",
    "        self.b_tilde = nn.Embedding(vocab_size, 1)\n",
    "\n",
    "    def forward(self, i_ids, j_ids):\n",
    "        wi = self.w(i_ids)                  # [B, D]\n",
    "        wj = self.w_tilde(j_ids)            # [B, D]\n",
    "        bi = self.b(i_ids).squeeze(1)       # [B]\n",
    "        bj = self.b_tilde(j_ids).squeeze(1) # [B]\n",
    "        return (wi * wj).sum(dim=1) + bi + bj  # [B]\n",
    "\n",
    "glove = GloVe(vocab_size=vocab_size, emb_dim=EMB_DIM).to(device)\n",
    "opt = optim.Adagrad(glove.parameters(), lr=LR_G)\n",
    "\n",
    "# Precompute weights f(X)\n",
    "fx = np.minimum((counts / X_MAX) ** ALPHA, 1.0).astype(np.float32)\n",
    "logx = np.log(counts + 1e-8).astype(np.float32)\n",
    "\n",
    "pairs_t = torch.tensor(pairs, dtype=torch.long, device=device)\n",
    "fx_t = torch.tensor(fx, dtype=torch.float32, device=device)\n",
    "logx_t = torch.tensor(logx, dtype=torch.float32, device=device)\n",
    "\n",
    "t0 = time.time()\n",
    "glove.train()\n",
    "\n",
    "for epoch in range(EPOCHS_G):\n",
    "    perm = torch.randperm(N, device=device)\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for start in range(0, N, BATCH_SIZE_G):\n",
    "        idx = perm[start:start+BATCH_SIZE_G]\n",
    "        ij = pairs_t[idx]\n",
    "        i_ids = ij[:, 0]\n",
    "        j_ids = ij[:, 1]\n",
    "\n",
    "        pred = glove(i_ids, j_ids)                 # [B]\n",
    "        diff = pred - logx_t[idx]                  # [B]\n",
    "        loss = (fx_t[idx] * diff * diff).mean()\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        total_loss += float(loss.item())\n",
    "        num_batches += 1\n",
    "\n",
    "    avg_loss = total_loss / max(1, num_batches)\n",
    "    print(f\"epoch {epoch+1}/{EPOCHS_G} avg_loss={avg_loss:.4f}\")\n",
    "\n",
    "t1 = time.time()\n",
    "train_time_glove = t1 - t0\n",
    "print(\"\\nGloVe DONE\")\n",
    "print(\"time_sec:\", train_time_glove)\n",
    "\n",
    "# Common final embedding = w + w_tilde\n",
    "glove_embeddings = (glove.w.weight.detach() + glove.w_tilde.weight.detach()).to(\"cpu\").numpy()\n",
    "np.save(\"glove_embeddings.npy\", glove_embeddings)\n",
    "print(\"saved: glove_embeddings.npy\", glove_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e935a014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded shapes: (4075, 50) (4075, 50) (4075, 50)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load embeddings you already saved\n",
    "E_softmax = np.load(\"sg_softmax_embeddings.npy\")\n",
    "E_neg = np.load(\"sg_neg_embeddings.npy\")\n",
    "E_glove = np.load(\"glove_embeddings.npy\")\n",
    "\n",
    "print(\"Loaded shapes:\", E_softmax.shape, E_neg.shape, E_glove.shape)\n",
    "\n",
    "def normalize_rows(M, eps=1e-9):\n",
    "    norms = np.linalg.norm(M, axis=1, keepdims=True)\n",
    "    return M / (norms + eps)\n",
    "\n",
    "# normalized versions for fast cosine\n",
    "E_softmax_n = normalize_rows(E_softmax)\n",
    "E_neg_n = normalize_rows(E_neg)\n",
    "E_glove_n = normalize_rows(E_glove)\n",
    "\n",
    "def analogy_accuracy(questions, E_n, word2id, topk=1):\n",
    "    \"\"\"\n",
    "    questions: list of (a,b,c,d) where a:b :: c:d\n",
    "    Return accuracy@topk over questions that are all in vocab.\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    used = 0\n",
    "    for a,b,c,d in questions:\n",
    "        if a not in word2id or b not in word2id or c not in word2id or d not in word2id:\n",
    "            continue\n",
    "        ia, ib, ic, id_ = word2id[a], word2id[b], word2id[c], word2id[d]\n",
    "        # v = b - a + c\n",
    "        v = E_n[ib] - E_n[ia] + E_n[ic]\n",
    "        v = v / (np.linalg.norm(v) + 1e-9)\n",
    "\n",
    "        sims = E_n @ v  # cosine because normalized\n",
    "        # exclude input words\n",
    "        sims[[ia, ib, ic]] = -1e9\n",
    "        # topk predictions\n",
    "        pred_ids = np.argpartition(-sims, topk)[:topk]\n",
    "        if id_ in pred_ids:\n",
    "            correct += 1\n",
    "        used += 1\n",
    "    acc = correct / used if used > 0 else 0.0\n",
    "    return acc, used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6197fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gensim version: 4.4.0\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "print(\"gensim version:\", gensim.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e79d37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordSim353 path: /Users/thetsusann/Desktop/NLP/Assignment1/.venv/lib/python3.12/site-packages/gensim/test/test_data/wordsim353.tsv\n",
      "Columns: ['# The WordSimilarity-353 Test Collection (http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/)']\n",
      "                # The WordSimilarity-353 Test Collection (http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/)\n",
      "# Word 1 Word 2                                       Human (mean)                                                      \n",
      "love     sex                                                  6.77                                                      \n",
      "tiger    cat                                                  7.35                                                      \n",
      "         tiger                                               10.00                                                      \n",
      "book     paper                                                7.46                                                      \n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import datapath\n",
    "import pandas as pd\n",
    "\n",
    "ws_path = datapath(\"wordsim353.tsv\")\n",
    "df_ws = pd.read_csv(ws_path, sep=\"\\t\")\n",
    "\n",
    "print(\"WordSim353 path:\", ws_path)\n",
    "print(\"Columns:\", list(df_ws.columns))\n",
    "print(df_ws.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "153b21ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordSim pairs ready: 353\n",
      "Sample: [('love', 'sex', 6.77), ('tiger', 'cat', 7.35), ('tiger', 'tiger', 10.0), ('book', 'paper', 7.46), ('computer', 'keyboard', 7.62)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import datapath\n",
    "\n",
    "ws_path = datapath(\"wordsim353.tsv\")\n",
    "\n",
    "wordsim_pairs = []\n",
    "with open(ws_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line or line.startswith(\"#\"):\n",
    "            continue\n",
    "        parts = line.split()\n",
    "        if len(parts) != 3:\n",
    "            continue\n",
    "        w1, w2, score = parts\n",
    "        wordsim_pairs.append((w1.lower(), w2.lower(), float(score)))\n",
    "\n",
    "print(\"WordSim pairs ready:\", len(wordsim_pairs))\n",
    "print(\"Sample:\", wordsim_pairs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3473fddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordSim353 Spearman:\n",
      "Skip-gram Softmax: 0.1183 (pairs used=105)\n",
      "Skip-gram NEG    : 0.0794 (pairs used=105)\n",
      "GloVe            : -0.0521 (pairs used=105)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "import numpy as np\n",
    "\n",
    "def wordsim_spearman(wordsim_pairs, E_n, word2id):\n",
    "    human_scores = []\n",
    "    model_scores = []\n",
    "\n",
    "    for w1, w2, score in wordsim_pairs:\n",
    "        if w1 not in word2id or w2 not in word2id:\n",
    "            continue\n",
    "        i, j = word2id[w1], word2id[w2]\n",
    "        sim = float(np.dot(E_n[i], E_n[j]))  # cosine because normalized\n",
    "        human_scores.append(score)\n",
    "        model_scores.append(sim)\n",
    "\n",
    "    corr, _ = spearmanr(human_scores, model_scores)\n",
    "    return corr, len(human_scores)\n",
    "\n",
    "# Compute for all 3 models\n",
    "ws_softmax, n1 = wordsim_spearman(wordsim_pairs, E_softmax_n, word2id)\n",
    "ws_neg, n2     = wordsim_spearman(wordsim_pairs, E_neg_n, word2id)\n",
    "ws_glove, n3   = wordsim_spearman(wordsim_pairs, E_glove_n, word2id)\n",
    "\n",
    "print(\"WordSim353 Spearman:\")\n",
    "print(f\"Skip-gram Softmax: {ws_softmax:.4f} (pairs used={n1})\")\n",
    "print(f\"Skip-gram NEG    : {ws_neg:.4f} (pairs used={n2})\")\n",
    "print(f\"GloVe            : {ws_glove:.4f} (pairs used={n3})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca07663a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total analogy questions: 19544\n",
      "Google Analogy Accuracy:\n",
      "Skip-gram Softmax: 0.0020 (used=1005)\n",
      "Skip-gram NEG    : 0.0020 (used=1005)\n",
      "GloVe            : 0.0000 (used=1005)\n"
     ]
    }
   ],
   "source": [
    "# Load analogy questions\n",
    "questions = []\n",
    "with open(analogy_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip().lower()\n",
    "        if not line or line.startswith(\":\"):\n",
    "            continue\n",
    "        a, b, c, d = line.split()\n",
    "        questions.append((a, b, c, d))\n",
    "\n",
    "print(\"Total analogy questions:\", len(questions))\n",
    "\n",
    "# Evaluate\n",
    "acc_softmax, used1 = analogy_accuracy(questions, E_softmax_n, word2id)\n",
    "acc_neg, used2     = analogy_accuracy(questions, E_neg_n, word2id)\n",
    "acc_glove, used3   = analogy_accuracy(questions, E_glove_n, word2id)\n",
    "\n",
    "print(\"Google Analogy Accuracy:\")\n",
    "print(f\"Skip-gram Softmax: {acc_softmax:.4f} (used={used1})\")\n",
    "print(f\"Skip-gram NEG    : {acc_neg:.4f} (used={used2})\")\n",
    "print(f\"GloVe            : {acc_glove:.4f} (used={used3})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "12d9858b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw paragraphs: 1998\n",
      "Kept paragraphs: 1998\n",
      "Paragraph vectors shape: (1998, 50)\n",
      "Saved: paragraph_vectors.npy and paragraph_texts.json\n",
      "Example paragraph: ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RIFT\n",
      "  Mounting trade friction between the\n",
      "  U.S. And Japan has raised fears among many of Asia's exporting\n",
      "  nations that the row could inflict far-reachin ...\n"
     ]
    }
   ],
   "source": [
    "import re, json\n",
    "import numpy as np\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "# Choose which embeddings to use for the web app\n",
    "# Recommended: NEG\n",
    "E = E_neg  # change to E_softmax or E_glove if you want\n",
    "\n",
    "# Normalize for cosine similarity later\n",
    "def normalize_rows(M, eps=1e-9):\n",
    "    norms = np.linalg.norm(M, axis=1, keepdims=True)\n",
    "    return M / (norms + eps)\n",
    "\n",
    "E_n = normalize_rows(E)\n",
    "\n",
    "def text_to_ids(text, word2id):\n",
    "    # simple tokenizer consistent with earlier: alphabetic lowercase\n",
    "    tokens = re.findall(r\"[a-zA-Z]+\", text.lower())\n",
    "    return [word2id.get(t, 0) for t in tokens]  # 0 = <UNK>\n",
    "\n",
    "def paragraph_vector(text, E, word2id):\n",
    "    ids = text_to_ids(text, word2id)\n",
    "    ids = [i for i in ids if i != 0]  # drop UNK for paragraph vector\n",
    "    if len(ids) == 0:\n",
    "        return None\n",
    "    return E[ids].mean(axis=0)\n",
    "\n",
    "# Build paragraph list from Reuters\n",
    "# Keep it modest to stay fast and keep web app snappy.\n",
    "MAX_DOCS = 2000   # increase later if you want\n",
    "MIN_CHARS = 80    # ignore tiny paragraphs\n",
    "\n",
    "fileids = reuters.fileids()[:MAX_DOCS]\n",
    "\n",
    "paragraphs = []\n",
    "for fid in fileids:\n",
    "    raw = reuters.raw(fid)\n",
    "    # split on blank lines; Reuters often has short lines, so filter by length\n",
    "    for p in re.split(r\"\\n\\s*\\n\", raw):\n",
    "        p = p.strip()\n",
    "        if len(p) >= MIN_CHARS:\n",
    "            paragraphs.append(p)\n",
    "\n",
    "print(\"Raw paragraphs:\", len(paragraphs))\n",
    "\n",
    "# Compute vectors\n",
    "vecs = []\n",
    "texts = []\n",
    "for p in paragraphs:\n",
    "    v = paragraph_vector(p, E, word2id)\n",
    "    if v is None:\n",
    "        continue\n",
    "    vecs.append(v.astype(np.float32))\n",
    "    texts.append(p)\n",
    "\n",
    "V = np.vstack(vecs)  # [N, D]\n",
    "V_n = normalize_rows(V)\n",
    "\n",
    "print(\"Kept paragraphs:\", len(texts))\n",
    "print(\"Paragraph vectors shape:\", V_n.shape)\n",
    "\n",
    "# Save\n",
    "np.save(\"paragraph_vectors.npy\", V_n)\n",
    "with open(\"paragraph_texts.json\", \"w\") as f:\n",
    "    json.dump(texts, f)\n",
    "\n",
    "print(\"Saved: paragraph_vectors.npy and paragraph_texts.json\")\n",
    "print(\"Example paragraph:\", texts[0][:200], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e5b346e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: word2id.json (size: 4075 )\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"word2id.json\", \"w\") as f:\n",
    "    json.dump(word2id, f)\n",
    "\n",
    "print(\"Saved: word2id.json (size:\", len(word2id), \")\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
